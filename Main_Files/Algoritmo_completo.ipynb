{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas que iremos utilizar:\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Início da Declaração de funções ---------------\n",
    "def marque_negacao(texto):\n",
    "    negacoes = ['não','not']\n",
    "    negacao_detectada = False\n",
    "    resultado = []\n",
    "    palavras = texto.split()\n",
    "    for p in palavras:\n",
    "        p = p.lower()\n",
    "        if negacao_detectada == True:\n",
    "            p = p + '_NEG'\n",
    "        if p in negacoes:\n",
    "            negacao_detectada = True\n",
    "        resultado.append(p)\n",
    "    return (\" \".join(resultado))\n",
    "\n",
    "def Preprocessing(instancia):\n",
    "    instancia = re.sub(r\"http\\S+\", \"\", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','').replace('\"','')\n",
    "    instancia = remove_emoji(instancia)\n",
    "    instancia = Lemmatization(instancia)\n",
    "    instancia  = Stemming(instancia)\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))\n",
    "\n",
    "def Lemmatization(instancia):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    palavras = []\n",
    "    for w in instancia.split():\n",
    "        palavras.append(wordnet_lemmatizer.lemmatize(w))\n",
    "    return (\" \".join(palavras))\n",
    "\n",
    "def Limpeza_dados(instancia):\n",
    "    # remove links, pontos, virgulas,ponto e virgulas dos tweets\n",
    "    instancia = re.sub(r\"http\\S+\", \"\", instancia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')\n",
    "    return (instancia)\n",
    "\n",
    "# Aplicando o stemming em nossa base:\n",
    "def Stemming(instancia):\n",
    "    stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "    palavras = []\n",
    "    for w in instancia.split():\n",
    "        palavras.append(stemmer.stem(w))\n",
    "    return (\" \".join(palavras))\n",
    "\n",
    "# Função para remover Stopwords da nossa base:\n",
    "def RemoveStopWords(instancia):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))\n",
    "\n",
    "# Função para remover os emojis do texto by: https://stackoverflow.com/a/49146722/330558 &\n",
    "# https://www.reddit.com/r/learnpython/comments/8br5sz/removing_emojis_from_words_python3/dx91wrm/\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U00010000-\\U0010ffff\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "# Função para mostrar os resultados do classificador\n",
    "def Metricas(modelo, tweets, classes):\n",
    "    resultados = cross_val_predict(modelo, tweets, classes, cv=10)\n",
    "    # Matriz de confusão:\n",
    "    print (pd.crosstab(classes, resultados, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "    print (metrics.classification_report(classes,resultados))\n",
    "    return 'Acurácia do modelo: {}'.format(metrics.accuracy_score(classes,resultados))\n",
    "\n",
    "# ----------- Final da Declaração de funções ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abre o arquivo correspondente\n",
    "#RODAR ESSA CELULA APENAS SE NÃO TIVER O ARQUIVO CSV DA DATABASE\n",
    "file = open(\"todos_comentarios.txt\", 'r', encoding=\"UTF-8\")\n",
    "# cria um pandas.DataFrame para armazena os resultados\n",
    "df = pd.DataFrame(columns=[\"Avaliação\", \"Comentário\"])\n",
    "linha = file.readline()\n",
    "\n",
    "while linha != \"\":\n",
    "    linhaSep = linha.split(\",\", maxsplit=1)\n",
    "    # separa toda a string da avaliação em uma lista, em que cada index é uma avaliação\n",
    "    Avalliation = linhaSep[0]\n",
    "    Comment = linhaSep[1]\n",
    "    # escreve em cada coluna do arquivo a sua avaliação e seu respectivo comentário.\n",
    "    df = df.append({\"Avaliação\": Avalliation, \"Comentário\": Comment}, ignore_index=True)\n",
    "    linha = file.readline()  # manuntenção do loop\n",
    "\n",
    "df.to_csv(\"todos_comentarios.csv\", index=False)  # salvando o csv\n",
    "print(\"Arquivos Formatados para csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo a base de dados:\n",
    "df = pd.read_csv('todos_comentarios.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31633"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando novamente o número de linhas dessa coluna:\n",
    "df.Comentário.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['Comentário']\n",
    "classes = df['Avaliação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [Preprocessing(i) for i in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o objeto que faz a vetorização dos dados de texto:\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_tweets = vectorizer.fit_transform(tweets)\n",
    "type(freq_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[0]+\"\\n\"+freq_tweets[0]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31633, 28992)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57220, 28992)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rodar essa celula se quiser OVERsampling\n",
    "bl = SMOTE()\n",
    "freq_tweets,classes = bl.fit_resample(freq_tweets, classes)\n",
    "freq_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rodar essa celula se quiser UNDERsampling\n",
    "bl = NearMiss()\n",
    "freq_tweets,classes = bl.fit_resample(freq_tweets, classes)\n",
    "freq_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificador NB\n",
    "nbClassifier = Pipeline([('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificador SVM:\n",
    "pipeline_svm_simples = Pipeline([('classifier', svm.SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificador LR\n",
    "pipeline_LR = Pipeline([('classifier',LogisticRegression(random_state=0, solver='newton-cg') )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline que atribui tag de negações nas palavras:\n",
    "pipeline_negacoes = Pipeline([\n",
    "  ('counts', CountVectorizer(tokenizer=lambda text: marque_negacao(text))),\n",
    "  ('classifier', MultinomialNB())\n",
    "])\n",
    "pipeline_negacoes.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline com tag de negação:\n",
    "pipeline_svm_negacoes = Pipeline([\n",
    "  ('counts', CountVectorizer(tokenizer=lambda text: marque_negacao(text))),\n",
    "  ('classifier', svm.SVC(kernel='linear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito      1      2     3      4      5    All\n",
      "Real                                            \n",
      "1         6576   2250  1071    660    887  11444\n",
      "2         3448   4130  1122   1818    926  11444\n",
      "3         2414   2385  2451   2857   1337  11444\n",
      "4         1251   1616  1250   4277   3050  11444\n",
      "5         1000    344   676   2777   6647  11444\n",
      "All      14689  10725  6570  12389  12847  57220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.57      0.50     11444\n",
      "           2       0.39      0.36      0.37     11444\n",
      "           3       0.37      0.21      0.27     11444\n",
      "           4       0.35      0.37      0.36     11444\n",
      "           5       0.52      0.58      0.55     11444\n",
      "\n",
      "    accuracy                           0.42     57220\n",
      "   macro avg       0.41      0.42      0.41     57220\n",
      "weighted avg       0.41      0.42      0.41     57220\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Acurácia do modelo: 0.42084935337294654'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metricas do Naive Bayes simples:\n",
    "Metricas(nbClassifier,freq_tweets,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metricas do LR simples:\n",
    "Metricas(pipeline_LR,freq_tweets,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metricas do SVM linear simples:\n",
    "Metricas(pipeline_svm_simples,freq_tweets,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes com tag de negacoes:\n",
    "Metricas(pipeline_negacoes,tweets,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM linear com tag de negacoes:\n",
    "Metricas(pipeline_svm_negacoes,tweets,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation:\n",
    "resultados = cross_val_predict(pipeline_negacoes, tweets, classes, cv=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
